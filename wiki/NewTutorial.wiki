#summary Our best tutorial for dumpgenerator.py
#labels Featured
<img src="http://upload.wikimedia.org/wikipedia/commons/f/f3/Nuvola_apps_Wild.png" width=100px alt="Documentation" title="Documentation" align=right/>
This is a tutorial to learn how to backup a wiki using WikiTeam tools. You can *backup your wiki or any public wiki*.

Table of contents:
<wiki:toc />

= Generating a wiki backup =
==I have shell access to server==

If you have shell access on the web server, follow the instructions on this page: http://www.mediawiki.org/wiki/Manual:DumpBackup.php for an XML backup. For an image backup, just copy the image directory (see your !LocalSettings.php for details).

==I have no shell access to server==

If you have no shell access, then use the WikiTeam [http://code.google.com/p/wikiteam/source/browse/trunk/dumpgenerator.py dumpgenerator.py] ([http://wikiteam.googlecode.com/svn/trunk/dumpgenerator.py download]), available at the [http://code.google.com/p/wikiteam/source/browse/#svn%2Ftrunk repository].

===Requirements===

You will need *Python*: http://www.python.org/download/
The dump generator will need to be run from a DOS or GNU/Linux command-line.

===Dump types===
There are two types of backups that can be made: *XML dumps* (current and history) and *image dumps*. (But you can do both in one dump - see below.)

An *XML dump* contains the meta-data of the edits (author, date, comment) and the text (wikitext). An XML dump may be "*current*" or "*history*". A "history" dump contains the complete history of every page, which is better for historical and research purposes and is the default. A "current" dump contains only the last edit for every page. 

An *image dump* contains all the images available in a wiki, plus their descriptions.

===Running the script===

There are two ways to start a backup - *API* (api.php) or *index.php* . API is the better method to use, but index.php can be used when the API is not available. (Not all mediawiki wikis have api.php.)
To find out if the wiki you want to back up has api.php or not, open a browser window and go to the wiki's Main Page.
Click on the "View history" tab. You will see a URL such as this: _http://en.wikipedia.org/w/index.php?title=Main_Page&action=history_
Now edit the URL: remove everything after _/w/_, and replace it with _api.php_ (In the example, the result would be: _http://en.wikipedia.org/w/api.php_). If you see a webpage with the API documentation, copy the URL to the clipboard. If not,  the API is probably not being used in the wiki or check the URL and try again (or use index.php).

If there is no api.php, then you'll need to use index.php. For the correct URL, in the example above, remove everything after the '_?_' and copy the URL to the clipboard. (In the example above the correct URL would be: _http://en.wikipedia.org/w/index.php_)

Note: *DO NOT* try to dump the Wikipedia site! It would result in a file that would quite likely be in the terabyte range. We are just using the Wikipedia URL as an example.

At a DOS prompt or GNU/Linux command line, type the following:

If the wiki you want to backup has api.php, use this:
  * _python dumpgenerator.py --api=http://en.wikipedia.org/w/api.php --xml_

If you need to use index.php, then use this:
  * _python dumpgenerator.py --index=http://en.wikipedia.org/w/index.php --xml_

For a complete dump with both xml and images, use both '--xml' and '--images' in the same command, like this:
  * _python dumpgenerator.py --api=http://en.wikipedia.org/w/api.php --xml --images_

The _--xml_ option gets the full history of every page. If you only want the last revision for every page, you will need to use _--xml --curonly_

To reduce the demands of downloading wiki pages one after another, it is recommended that you use a delay. 
Use _--delay_ to add any length of delay you want. For example, a delay of 5 seconds between each request: _--delay=5_

To resume an interrupted dump, use the parameters _--resume_ and _--path_ to indicate where the incomplete dump is located. The command will look something like this:
  * _python dumpgenerator.py --api=http://en.wikipedia.org/w/index.php --xml --images --resume --path=dumpdirectory_

When you start a new dump, the script will create a directory (or folder) in the form of: domainorg-20110520-wikidump . Sometimes there will be a sub-directory in the name, or "wiki" added in - that's OK. 

===Check dump integrity===

If you want to check the XML dump integrity (optional), there are two possible methods:

1. You can try to import it into a MediaWiki (using [http://www.mediawiki.org/wiki/Manual:Importing_XML_dumps this method], which may be slow if wiki is large).

or

2. Type this into your command line (GNU/Linux) to count _`<title>`__`<page>`__`</page>`_ and _`<revision>`__`</revision>`_ XML tags:

  * _grep "`<title>`" `*`.xml -c;grep "`<page>`" `*`.xml -c;grep "`</page>`" `*`.xml -c;grep "`<revision>`" `*`.xml -c;grep "`</revision>`" `*`.xml -c___

You should see something similar to this (not the actual numbers - the first three numbers should be the same and the last two should be the same as each other):
  * 580
  * 580
  * 580
  * 5677
  * 5677

If your first three numbers or your last two numbers are different, then, your XML dump is corrupt (it contains one or more unfinished `</page>` or `</revision>`). This is not common in small wikis, but large or very large wikis may fail at this due to truncated XML pages while exporting and merging. The solution is to remove the XML dump and resume (re-download, a bit boring, and it can fail again...), or if you don't care to lose the corrupt pages, exclude them with this script (TO DO).

===Further steps: compression===
After the wiki dump is complete, you will need to archive the files in the wikidump directory. The WikiTeam project uses the *7-Zip* file archiver. 7z is a compressed archive file format. We use it because it is free (the GNU LGPL license) and it has a high compression ratio. [http://en.wikipedia.org/wiki/7z Read about it] in Wikipedia. There is  software for [http://www.7-zip.org Windows] and [http://p7zip.sourceforge.net GNU/Linux] (_sudo apt-get install p7zip_). There are also graphical front-ends for archiving programs for both Windows and GNU/Linux. 

You can use a graphical front-end to compress and archive the files, or run one of these commands:
  * 7z a  domainorg-20110520-history.xml.7z domainorg-20110520-wikidump/*  -- for xml only

or

  * 7z a  domainorg-20110520-complete.7z domainorg-20110520-wikidump/* -- for xml & images

It is suggested that you use the following file-naming convention: domainorg-20110520-history.xml.7z - where "domainorg" is the domain name, "20110520" is the date the dump was started. (You can use the domain name and date from the directory that was created when the dump was started.)

=Publishing the dump=
And of course, you then have to release your dumps! You can easily do so on the [http://www.archive.org Internet Archive], as with [http://www.archive.org/details/wiki-artandpopularculture.com this example]. 
  # [http://www.archive.org/account/login.php Login] and [http://www.archive.org/create/ create a new item] for each wiki. 
  # Use wiki-DOMAIN as identifier and "Wiki - NAME" as title. 
  # Add "MediaWiki; wiki; wikiteam" as subject/keywords so that it's possible to find the item. 
  # If possible, enter more info in the relevant fields, like the license of the wiki. 
  # Follow the instructions to upload the archives. 
  # In the last step, put the item in texts/opensource (community texts); it will then moved to the WikiTeam collection (until then, it should be listed with this [http://www.archive.org/search.php?query=subject%3A%22wikiteam%22%20AND%20collection%3Aopensource search].

Advanced tips: 
  # For particularly big archives you might want to use [http://www.archive.org/help/abouts3.txt S3] (you may need to request authorisation to info at archive dot org), or [https://wiki.archive.org/twiki/bin/view/Main/IAS3BulkUploader this tool] if you have many items. 
  # If you use such command line tools, choose "web" as media type: it's more correct, doesn't need authorisation and will make the items appear in the [http://www.archive.org/details/web web crawls collection].
  # Beware that if you specify a non-Creative Commons license in the licenseurl parameter, due to a bug, you won't be able to edit the item metadata via the graphical interface any longer, so put all the details from the start.

=I want to import an XML/image dump, how can I do it?=

Read http://www.mediawiki.org/wiki/Manual:Importing_XML_dumps and http://www.mediawiki.org/wiki/Manual:ImportImages.php.

=Further help=
Remember you always can see the inline help with _--help_ parameter.

If you have further questions, you can send a message to our [http://groups.google.com/group/wikiteam-discuss mailing list]. If you detect any error, [http://code.google.com/p/wikiteam/issues/list report an issue]. *Be bold!*