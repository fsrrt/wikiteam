Here, we coordinate us to download all *non WikiFarms* wikis.

We start with a [http://www.cs.brown.edu/~pavlo/mediawiki/ list from Andrew Pavlo], which is [http://code.google.com/p/wikiteam/source/browse/trunk/listsofwikis/mediawikis_pavlo.csv mirrored here]. After discarding dead wikis, we have about ~XXXX alive wikis.

== How to join the effort ==
Download a list (every list has 100 wikis) and use the following scripts to backup the wikis:

  * [http://code.google.com/p/wikiteam/source/browse/trunk/batchdownload/launcher.py launcher.py]
  * [http://code.google.com/p/wikiteam/source/browse/trunk/dumpgenerator.py dumpgenerator.py]

After downloading a list and both scripts in the same directory, you can do: *python launcher.py mylist.txt*

It will backup every wiki and generate a _-history.xml.7z_ (just wikitext) and _-wikidump.7z_ (wikitext and images/ directory).