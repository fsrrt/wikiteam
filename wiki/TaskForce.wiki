#summary Help us archiving a gallizion of wikis
Here, we coordinate us to download all *non WikiFarms* wikis.

We start with a [http://www.cs.brown.edu/~pavlo/mediawiki/ list from Andrew Pavlo], which is [http://code.google.com/p/wikiteam/source/browse/trunk/listsofwikis/mediawikis_pavlo.csv mirrored here]. After discarding dead wikis, we have about 7,000+ alive wikis.

== Requirements ==
You need GNU/Linux, Python and p7zip-full (sudo apt-get install p7zip-full).

== How to join the effort ==

Download a list (every list has 100 wikis):

  * Choose one from [http://code.google.com/p/wikiteam/source/browse/trunk#trunk%2Fbatchdownload%2Flists 000 to 073]

And use the following scripts to backup the wikis:

  * [http://wikiteam.googlecode.com/svn/trunk/batchdownload/launcher.py launcher.py]
  * [http://wikiteam.googlecode.com/svn/trunk/dumpgenerator.py dumpgenerator.py]

After downloading a list and both scripts in the same directory, you can do: *python launcher.py mylist.txt*

It will backup every wiki and generate two 7z files for each one: _-history.xml.7z_ (just wikitext) and _-wikidump.7z_ (wikitext and images/ directory).

=== Volunteers table ===

Lists claimed by users.

|| 000 || emijrp ||
|| 001 || ... ||
|| 002 || ... ||

== Where/How to upload the dumps? ==

We will upload the dumps to the [http://archive.org/details/wikiteam WikiTeam Collection] at Internet Archive. Please, wait further instructions.