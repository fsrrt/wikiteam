#summary Help us to archive a gallizion of wikis
#labels Featured
#sidebar Sidebar
Here, we coordinate us to download all *non WikiFarms* wikis.

We start with a [http://www.cs.brown.edu/~pavlo/mediawiki/ list from Andrew Pavlo], which is [http://code.google.com/p/wikiteam/source/browse/trunk/listsofwikis/mediawikis_pavlo.csv mirrored here]. After discarding dead wikis, we have about [http://code.google.com/p/wikiteam/source/browse/trunk/listsofwikis/mediawikis_pavlo.alive.filtered.txt 7,400+ alive wikis].

== Requirements ==
You need GNU/Linux, Python and p7zip-full (sudo apt-get install p7zip-full).

== How to join the effort? ==

Download a list (every list has 100 wikis):

  * Choose one from [http://code.google.com/p/wikiteam/source/browse/trunk#trunk%2Fbatchdownload%2Flists 000 to 073]

And use the following scripts to backup the wikis:

  * [http://wikiteam.googlecode.com/svn/trunk/batchdownload/launcher.py launcher.py]
  * [http://wikiteam.googlecode.com/svn/trunk/dumpgenerator.py dumpgenerator.py]

After downloading a list and both scripts in the same directory, you can do: *python launcher.py mylist.txt*

It will backup every wiki and generate two 7z files for each one: _domain-date-history.xml.7z_ (just wikitext) and _domain-date-wikidump.7z_ (wikitext and images/ directory).

See also the NewTutorial page for details on how the scripts work.

*Note*: we recommend to split every 100 wikis list into 10 lists of 10. You can do that with the {{{split}}} command like this: {{{split -l10 list000 list000-}}}

=== Volunteers table ===

Lists claimed by users. Notify us when you start downloading a list of wikis. If you can't edit this page, email us in [https://groups.google.com/group/wikiteam-discuss our mailing list] (you have to join).

|| *List* || *Member* || *Status* || *Notes * ||
|| 000 || emijrp || Downloading || Downloaded: X. Incomplete: X. Errors: X. ||
|| 001 || ScottDB || Downloading || Working on a VERY large wiki in this list (376K page titles).<-- It's done. ... ||
|| 002 || underscor || Downloading || ... ||
|| 003 || Nemo || Downloading || Several big wikis here, including citywiki.ugr.es which seems to have about 100 GB of images. ||
|| 004 || ianmcorvidae || Downloading || ... ||
|| 005 || Nemo || _Downloaded_ || [http://p.defau.lt/?b_5gXJxVTPPzppgd7jxfrg Final check] ||
|| 006 || Nemo || Downloading || ... ||
|| 007 || Nemo || _Downloaded_ || [http://p.defau.lt/?3qpqOZdAwinhcBFqCAg4vg Final check] ||
|| 008 || Nemo || _Downloaded_ || [http://p.defau.lt/?mS38BbpNQteTQZa8AI3mWQ Final check] ||
|| 009 || Nemo || _Downloaded_ || [http://p.defau.lt/?N4iuP5BFsmNjxf1wJ6GLDw Final check] ||
|| 010 || Nemo || Downloading || katlas.math.toronto.edu has 1.6 million pages, end expected for mid May. ||
|| 011 || mutoso || _Downloaded_ || ... ||
|| 012 || Nemo || _Downloaded_ || [http://p.defau.lt/?c_BiYqgcaqw3WHpIsYiZYg Final check] ||
|| 013 || Nemo || Downloading || ... ||
|| 014 || Nemo || _Downloaded_ || [http://p.defau.lt/?_23I5WxxA1Nl420d7e2w6g Final check] ||
|| 015 || Nemo || _Downloaded_ || [http://p.defau.lt/?KwIjaL3YnDFaLZAxguSz5g Final check] ||
|| 016 || Nemo || Downloading || ... ||
|| 017 || Nemo || _Downloaded_ || [http://p.defau.lt/?pacTBWaiBfOGWW00Qdzaew Final check] ||
|| 018 || ... || || ... ||
|| 019 || ... || || ... ||
|| 020 || mutoso || || ... ||
|| 021 || ... || || ... ||
|| 022 || ... || || ... ||
|| 023 || ... || || ... ||
|| 024 || ... || || ... ||
|| 025 || ... || || ... ||
|| 026 || ... || || ... ||
|| 027 || ... || || ... ||
|| 028 || ... || || ... ||
|| 029 || ... || || ... ||

== Where/How to upload the dumps? ==

We will upload the dumps to the [http://archive.org/details/wikiteam WikiTeam Collection] at Internet Archive. Please, wait for further instructions.